name: 'half_cheetah_ddpg'

env:
  name: vel.rl.env.mujoco
  game: 'HalfCheetah-v2'
#  normalize_returns: true
#  normalize_observations: true


vec_env:
  name: vel.rl.vecenv.dummy


model:
  name: vel.rl.models.deterministic_policy_model

  input_block:
    name: vel.modules.input.normalize_observations
    input_shape: 17

  policy_backbone:
    name: vel.rl.models.backbone.mlp
    input_length: 17
    hidden_layers: [64, 64]
    activation: 'tanh'
#    normalization: 'layer'

  value_backbone:
    name: vel.rl.models.backbone.mlp
    input_length: 23  # Has to be observation size(17) + action size(6)
    hidden_layers: [64, 64]
    activation: 'tanh'
#    normalization: 'layer'


reinforcer:
  name: vel.rl.reinforcers.buffered_off_policy_iteration_reinforcer

  env_roller:
    name: vel.rl.env_roller.transition_replay_env_roller

    replay_buffer:
      name: vel.rl.buffers.circular_replay_buffer

      buffer_capacity: 1_000_000
      buffer_initial_size: 2_000

    normalize_returns: true
    discount_factor: 0.99

    action_noise:
      name: vel.rl.modules.noise.ou_noise
      std_dev: 0.2

  algo:
    name: vel.rl.algo.policy_gradient.ddpg

    tau: 0.01
    discount_factor: 0.99

  rollout_steps: 2
  training_steps: 64

  parallel_envs: 1


optimizer:
  name: vel.optimizers.adam
  # OpenAI has two different optimizers optimizing each network separately.
  # As far as I know it should be equivalent to optimizing two separate networks together with a sum of loss functions
  lr: [1.0e-4, 1.0e-3, 1.0e-3]
  weight_decay: [0.0, 0.0, 0.01]
  epsilon: 1.0e-4
  layer_groups: on


commands:
  train:
    name: vel.rl.commands.rl_train_command
    total_frames: 1.0e6
    batches_per_epoch: 1000

#    openai_logging: true

  record:
    name: vel.rl.commands.record_movie_command
    takes: 10
    videoname: 'half_cheetah_vid_{:04}.avi'

  evaluate:
    name: vel.rl.commands.evaluate_env_command
    takes: 100
    frame_history: 4
